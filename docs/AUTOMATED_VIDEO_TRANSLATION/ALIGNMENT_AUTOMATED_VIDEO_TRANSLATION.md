# 对齐文档：自动化视频翻译 (AUTOMATED_VIDEO_TRANSLATION)

## 1. 原始需求

用户希望实现一个自动化的视频翻译功能（语音到语音）。具体要求如下：
- 输入一个包含中文对白视频文件。
- 将视频中的中文对白翻译成英文。
- 生成的英文语音需要保持原说话者的音色、语调和情感。
- 将视频中的原对白替换为翻译后的英文对白。
- 同时实现字幕的替换（即生成并嵌入新的英文字幕）。
- 最终输出一个经过完整语音翻译和字幕替换的视频文件。

## 2. 范围边界

根据现有项目工具和常规实践，定义以下范围：

**范围内 (In-Scope):**
- **视频处理**: 支持处理常见的视频格式（如 .mp4, .mkv）。
- **音频提取**: 从输入视频中自动提取完整的音轨。
- **人声/背景音分离**: 将人声对白与背景音乐/音效分离。
- **语音识别 (ASR)**: 使用 `whisperX` 将分离出的人声（中文）转换为带时间戳和说话人标识的文本。
- **原字幕处理 (新增)**: 检测并遮蔽视频中原有的硬字幕。
- **文本翻译**: 将识别出的中文文本翻译成英文。
- **语音克隆与合成 (TTS)**: 针对每个说话人，提取其声音样本，克隆音色，并合成翻译后的英文语音。
- **音频合并**: 将新生成的英文对白与原始的背景音乐/音效重新合并。
- **字幕生成**: 根据翻译后的文本和时间戳生成标准的 `.srt` 格式英文字幕。
- **最终视频生成**: 将新的音频轨道和新的**硬字幕**集成到原视频中，生成最终成品。
- **交付形式**: 一个可以通过命令行运行的Python脚本。

**范围外 (Out-of-Scope):**
- **实时翻译**: 不支持流式或实时处理。
- **视频内非字幕文字翻译**: 不翻译视频画面中出现的任何非字幕类文本（如路牌、标题等）。
- **图形用户界面 (UI)**: 初期版本不提供GUI，仅通过命令行操作。
- **多语言支持**: 初期版本仅支持中文到英文的翻译。

## 3. 需求理解

我的理解是，我们将构建一个自动化的后端处理流程。这个流程像一个管道，输入是一个视频文件，经过一系列AI模型的串联处理后，输出一个对白和字幕都变成英文的新视频。核心挑战在于实现高质量、跨语言的音色克隆，并保持情感一致性，以及准确地检测和遮蔽原有字幕。

整个流程将串联起项目已有的 `whisperX`（用于ASR），并集成最优的音源分离和语音克隆模块。

## 4. 已确认决策与待办任务

根据您的反馈，我们已确定以下几点，并明确了下一步需要执行的研究任务。

### 已确认决策
1.  **文本翻译服务**: **确定采用大模型API (如DeepSeek)**。在开发阶段，需要您提供API Key及相关的调用文档。
2.  **字幕集成方式**: **确定采用硬字幕 (Hardsub)**。
3.  **环境依赖**: **确认`ffmpeg`已安装**，可在代码中直接调用。

### 待办研究任务
1.  **音色克隆技术选型 (评估任务)**
    - **目标**: 对比 `index-tts` 和 `OpenVoice` 两个项目。
    - **评估维度**: 跨语言（中->英）的音色相似度、情感保持能力、对输入样本的要求、推理性能、集成复杂度。
    - **产出**: 一份技术评估报告，结论将用于最终的技术选型。

2.  **音源分离技术选型 (评估任务)**
    - **目标**: 对比 `audiocraft` 和 `audio_separator` 两个工具。
    - **评估维度**: 人声与背景音分离的质量、处理速度、资源消耗、模型鲁棒性。
    - **产出**: 一份技术评估报告，结论将用于最终的技术选型。

3.  **原字幕遮蔽技术 (研究任务)**
    - **目标**: 找到一个可靠的方案来自动检测视频中的硬字幕区域，并进行遮蔽。
    - **研究方向**: 这通常涉及到计算机视觉（CV）技术，可能需要使用OCR（光学字符识别）来识别文本区域，或者使用图像处理技术（如边缘检测、颜色对比）来定位字幕框。需要研究现有库（如 OpenCV）的可行方案。
    - **产出**: 一个可行的技术方案描述，用于后续的架构设计。

在完成以上研究任务后，我将为您制定一份包含最终技术选型和详细实施步骤的`CONSENSUS`（共识）文档。
