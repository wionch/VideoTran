# 任务对齐文档 (Alignment): AUTOMATED_VIDEO_TRANSLATION

## 修订说明 (Revision Note)
**注意：** 本文件记录了项目的初步需求对齐。在后续的详细设计（见 `DESIGN` 文档）中，为了确保系统的健壮性并从根本上避免‘声画错位’（即A说话B发声）的严重逻辑错误，原定的‘说话人日志 (Diarization)’方案被更稳定的‘孤岛’策略所取代。因此，本文档中关于处理多说话人的具体技术路径已被更新，请以 `DESIGN` 文档为准。

---

本文件旨在对“自动化视频语音翻译”任务的需求、范围和技术方案进行对齐，确保开发方向与您的预期完全一致。

## 1. 原始需求

用户提出一个自动化视频翻译（语音到语音）的实现思路，核心需求如下：

- **输入**: 一个包含中文对白视频A。
- **处理流程**:
    1. 提取音频 (`moviepy/ffmpeg`)。
    2. 人声/非人声分离 (`audio_separator`)。
    3. 人声转录为带时间戳的SRT字幕 (`faster-whisper/whisperx`)。
    4. OCR提取视频硬字幕，与SRT比对校正，并使用LLM进行优化 (`DEEPSEEK`)。
    5. 翻译字幕文本 (`DEEPSEEK`)。
    6. 使用翻译文本和原说话人音色生成新音频 (`OpenVoice2`)。
    7. 移除原音轨，合并新音轨。
    8. 将翻译后的字幕硬编码进视频。
- **输出**: 一个对白被替换为英文，且保留原说话人音色、语调、情感的视频，并带有新的英文字幕。

## 2. 需求理解与优化建议

您的思路非常全面且技术选型先进，完全具备可行性。为了使系统更加健壮和高质量，我基于您的想法进行了深化和优化，形成如下建议工作流：

### 2.1. 优化后的核心工作流

1.  **初始化**:
    -   接收输入（视频文件、源语言、目标语言）。
    -   为该视频任务创建独立的临时工作目录。

2.  **音频分析与转录**:
    -   **提取音频**: 从视频中完整提取音频 (`ffmpeg`)。
    -   **音轨分离**: 将音频分离为人声（vocals）和背景声（background）两轨 (`audio_separator`)。
    -   **说话人日志 (Diarization)**: **(关键补充点)** 使用 `whisperX` 对人声轨道进行处理，不仅转录文本，还要识别出**是谁在说话**。输出格式为 `(开始时间, 结束时间, 说话人ID, 文本)`。这是实现多角色音色克隆的基础。

3.  **文本处理**:
    -   **ASR校正 (可选)**: 使用LLM（如DEEPSEEK/GPT-4）对ASR（自动语音识别）生成的文本进行润色，修正标点和语法错误。
    -   **文本翻译**: 调用LLM将校正后的文本逐句翻译成目标语言。

4.  **语音合成 (Voice Cloning)**:
    -   **音色提取**: **(关键补充点)** 根据步骤2的说话人ID，从人声轨道中自动为**每一位**说话人提取一段清晰的语音作为音色参考（Reference Audio）。
    -   **语音生成**: 遍历所有文本片段，使用 `OpenVoice` 或 `XTTS`，并结合对应说话人的音色参考，生成翻译后的语音。
    -   **时长对齐 (关键优化点)**: 在生成语音时，尝试控制语速，使新生成音频的时长与原始语音片段的时长大致匹配，这是保证口型同步（Lip-sync）的基础。

5.  **音频后期合成**:
    -   根据时间戳，将所有新生成的语音片段拼接成一个完整的目标语言人声轨道。
    -   将新的目标语言人声轨道与原始的背景声轨道合并，生成最终的完整音轨。

6.  **视频终版制作**:
    -   使用 `ffmpeg` 将原视频的音轨移除。
    -   将上一步合成的全新音轨添加到视频中。
    -   (可选) 将翻译后的字幕文本以“软字幕”（可开关）或“硬字幕”（内嵌）的形式添加到视频中。

### 2.2. 组件选型评估

您选择的工具非常出色，我完全赞同。
- **音频/视频处理**: `ffmpeg` 是不二之选。
- **人声分离**: `audio_separator` (基于 UVR) 是目前效果最好的开源工具之一。
- **转录/说话人日志**: `whisperX` 是实现高精度时间戳和说话人识别的最佳选择。
- **翻译/校正**: `DEEPSEEK` 或其他顶尖LLM（如GPT-4, Gemini）是保证翻译质量的关键。
- **语音克隆**: `OpenVoice v2` 和 `XTTS v2` 是当前最顶尖的开源跨语言语音克隆模型，两者都可作为备选。

## 3. 范围边界 (Scope)

### 范围内 (In-Scope):
-   实现上述优化后工作流的自动化脚本。
-   支持处理单个视频文件。
-   支持中文到英文的翻译。
-   处理不多于5个清晰可辨的说话人。
-   生成一个带有翻译后音轨和可选字幕的视频文件。

### 范围外 (Out-of-Scope):
-   **完美的口型同步**: 精确的口型同步（Lip-sync）是世界级难题，本任务旨在做到时长对齐，而非像素级的口型匹配。
-   **完美的情感迁移**: 跨语言的情感精准复刻极具挑战，模型会尽力模仿，但无法保证100%还原。
-   **OCR字幕提取**: 您提到的OCR方案，虽然可行，但会引入额外的复杂度和不确定性（如OCR识别错误、字幕与语音时间轴对不齐等）。**建议在第一阶段将其作为可选的实验性功能，或暂时排除在核心流程外**，优先保证语音翻译流程的稳定。
-   **Web界面或批量处理**: 本任务聚焦于核心处理流程的脚本化实现。

## 4. 待澄清问题 (Points of Clarification)

在进入架构设计（Architect）阶段前，希望您能确认以下几点，以便我能设计出最符合您需求的方案：

1.  **关于OCR字幕校正**: 您是否同意我的建议，在第一版中**优先实现没有OCR的纯语音翻译流程**，将OCR作为后续的优化方向？这能让我们更快地得到一个可用的核心产品。
2.  **多说话人处理**: 我的方案补充了对多说话人的处理逻辑（使用`whisperX`识别ID并分别克隆音色）。这是否符合您的预期？
3.  **最终产出物**: 您希望最终交付的是一个可以执行的Python脚本（例如 `python translate_video.py --input video.mp4`），还是一个功能更完整的模块化代码库？
4.  **技术偏好**: 在语音克隆方面，`OpenVoice` 和 `XTTS` 效果都很好。您是否有特别的偏好？如果没有，我将基于实现复杂度和效果先选择其一。

请您审阅以上内容，特别是“待澄清问题”。您的反馈将是下一阶段架构设计的关键输入。
