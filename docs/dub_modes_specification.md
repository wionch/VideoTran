# 功能说明文档: `dub` vs `dub_v2` 模式

本文档详细解释了 `video_tran/tts_generator/run.py` 脚本中两种核心配音模式的工作流程和区别。这两种模式通过 `--align-duration` 命令行参数进行切换。

- **`dub` 模式**: 未设置 `--align-duration` 参数时的默认行为。
- **`dub_v2` 模式**: 设置 `--align-duration` 参数时启用的高级对齐模式。

## 共同流程

无论使用何种模式，以下流程是共享的：

1.  **音色提取**:
    -   如果使用 `--use-speaker-ref`，脚本会预加载每个说话人的音色。
    -   **音色回退机制**: 如果某个说话人（如 `SPEAKER_B`）的参考音频无效或过短导致提取失败，系统会自动使用上一个成功加载的音色（如 `SPEAKER_A` 的音色）作为备用，并打印警告信息。这确保了即使部分参考音有问题，也不会中断整个流程。
2.  **音色转换**: TTS模型首先生成一段使用基础音色的临时音频，然后 `ToneColorConverter` 会将这段音频的音色转换为目标说话人的音色。
3.  **音量标准化**: 在每个音频片段最终导出前，其响度都会被标准化到 `-20 dBFS`。这确保了所有片段音量的一致性，并解决了原始版本中音量过低的问题。
4.  **日志记录**: 在`dub_v2`模式下, 每个片段的处理过程都会生成详细的JSON格式日志，包含文本、参考音路径、目标时长、实际生成时长、最终速度等关键信息，便于调试。

---

## `dub` 模式 (默认, `--align-duration` 未设置)

此模式为基础配音模式，处理流程相对简单，速度较快。

### 工作流程

1.  **TTS生成**:
    -   对于每个文本片段，TTS模型以固定的 `speed=1.0` 生成语音。
    -   系统不会关心生成音频的时长是否与原始视频片段的时长匹配。

2.  **后处理**:
    -   生成的音频经过音色转换后，直接进行音量标准化。
    -   **无时长对齐**: 此模式下**不会**进行任何时长调整。既不会加速或减速音频，也不会填充静音。

### 优缺点

-   **优点**: 生成速度快，CPU开销小。
-   **缺点**: 生成的配音音轨与视频中的口型和时间点**很可能不匹配**，可能导致声画严重不同步。

---

## `dub_v2` 模式 (高级, `--align-duration` 已设置)

此模式旨在生成与原始视频时长精确对齐的配音音轨，以达到更好的声画同步效果。

### 工作流程

1.  **TTS生成 (两步时长控制)**:
    -   **首次生成**: TTS模型首先以 `speed=1.0` 的速度生成一遍音频，以获取其“自然”时长 `current_duration_ms`。
    -   **时长判断与调整**:
        -   **如果 `current_duration_ms` > `target_duration_ms` (生成音频比原视频长)**: 程序会计算出一个大于1.0的 `final_speed` (具体为 `current_duration_ms / target_duration_ms`)，并**重新生成**一次音频，以加快语速使其缩短到接近目标时长。
        -   **如果 `current_duration_ms` <= `target_duration_ms` (生成音频比原视频短或等长)**: 程序**不会**改变语速 (`final_speed` 保持为1.0)，并直接使用第一次生成的音频进入下一步。**不会为了拉长音频而放慢语速**。

2.  **后处理 (精确对齐)**:
    -   经过音色转换和语速调整（如果需要）的音频，会进入最终的对齐步骤。
    -   **静音填充**: 如果音频的时长仍然短于目标时长 `target_duration_ms`，程序会在音频的末尾**填充静音**，直到其总长度与 `target_duration_ms` 完全相等。
    -   在填充和标准化之后，最终的音频片段才会被导出。

### 优缺点

-   **优点**:
    -   通过加速长句和填充短句，最大限度地保证了最终合成的音轨与原始视频的时长**逐句对齐**。
    -   声画同步效果远优于 `dub` 模式。
-   **缺点**:
    -   处理流程更复杂，对于需要加速的片段，会进行两次TTS生成，因此CPU开销更大，处理速度相对较慢。
