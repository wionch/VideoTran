
# 需求对齐文档：提高说话人分离准确率

## 1. 原始需求
用户反馈由 `wpx.py` 生成的说话人分离结果（`output\Vocals_wpx_4.srt`）准确率只有60-70%。使用 `wpx2.py` 进行声纹匹配优化后的结果（`Vocals_wpx_1_refined.srt`）也未达到预期。

核心诉求：
1.  **代码排查**: 梳理 `wpx.py` 和 `wpx2.py` 的代码，判断逻辑和参数传递是否存在问题。
2.  **优化分析**: 评估现有代码在识别准确率上是否存在优化空间。
3.  **方案调研**: 寻找能显著提升说话人角色识别准确率的新方法、模型或方案。

## 2. 范围边界
- **范围内 (In-Scope)**:
    - 分析 `wpx.py` 和 `wpx2.py` 的代码逻辑。
    - 分析 `whisperX` 及其依赖（如 `pyannote.audio`）的调用方式和参数配置。
    - 识别并修复潜在的逻辑缺陷或参数错误。
    - 调研并提出改进说话人分离准确率的具体技术方案。
    - （可选）基于新方案实现一个概念验证（PoC）。

- **范围外 (Out-of-Scope)**:
    - 从头重新训练 `whisperX` 或 `pyannote.audio` 的核心模型。
    - 开发一个全新的、与现有技术栈无关的语音识别或说话人分离系统。
    - 对 `VideoTran` 项目中与说话人分离无关的其他部分（如TTS、音频生成）进行修改。

## 3. 需求理解
我的理解是，核心目标是提升最终生成的 SRT 字幕文件中说话人标签（如 `SPEAKER_00`, `SPEAKER_01`）的准确性。这需要分为两步：
1.  **诊断现有问题**：彻底检查当前代码实现，确保其发挥了所用工具（whisperX, pyannote）应有的性能，修复可能存在的错误。
2.  **探索更高上限**：在现有工具或方法的基础上，研究更先进的技术或更优的配置，以求突破当前方案的准确率瓶颈。

## 4. 需要澄清的问题点
*   **关于音频内容本身**:
    1.  您处理的音频中通常有几位说话人？这个数量是固定的（比如总是2人），还是会变化？（这直接关系到 `wpx.py` 中 `num_speakers=2` 这个硬编码参数是否合理）。
    2.  音频的质量如何？是否存在明显的背景噪音、音乐，或者说话人之间声音重叠的情况？

*   **关于 `wpx2.py` 的声纹样本**:
    3.  您用于 `wpx2.py` 的声纹样本文件（`a.wav`, `b.wav`）是如何创建的？它们是来自原始音频的、时长5-10秒左右的纯净人声片段吗？样本的清晰度如何？

*   **关于“不准确”的具体表现**:
    4.  当您说准确率只有60-70%时，典型的错误是哪种情况？
        *   A. 一整句话被完全分配给了错误的说话人。
        *   B. 一句话中间被错误地切开，分给了两个人。
        *   C. `wpx.py` 分离出的 `SPEAKER_00` 和 `SPEAKER_01` 标签本身是混乱的（即 `SPEAKER_00` 一会儿是A，一会儿是B）。
        *   D. `wpx2.py` 中，最终映射的名字是错的（比如把“嘉宾”的语音标成了“主持人”）。

*   **关于如何衡量改进效果**:
    5.  为了能客观地衡量我们后续方案的改进效果，您是否可以提供一个简短的（1-2分钟）音频或视频片段，并附带一个您手动校对好的、完全正确的SRT字幕文件作为“标准答案”？这将是验证新方案有效性的最佳方式。
