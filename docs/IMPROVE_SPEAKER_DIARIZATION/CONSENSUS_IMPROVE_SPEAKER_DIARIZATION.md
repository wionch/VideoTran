
# 共识文档：提高说话人分离准确率

## 1. 需求与问题分析概要

根据用户在对齐阶段提供的反馈，我们达成以下共识：

- **任务目标**: 显著提升双人对话场景下，SRT字幕中说话人标签的准确率。
- **音频特征**: 输入音频经过了人声分离，质量较高，且固定为2名说话人。
- **问题根源**: 当前 `wpx.py` 和 `wpx2.py` 两个脚本的准确率瓶颈，并非来自参数配置错误（如说话人数量）或音频质量问题。根本原因在于，两个脚本都依赖的 `pyannote.audio` **无监督**说话人分离（Diarization）算法，在处理当前音频时无法有效、稳定地区分两位说话人，导致了“整句话分配错误”的核心问题。`wpx2.py` 的“后处理映射”方案，由于其基础（无监督分离）存在错误，因此也无法达到预期效果。

## 2. 技术共识与实施方向

我们一致同意，解决此问题的最佳路径是 **放弃对现有无监督流程的修补，设计一个全新的、更可靠的“强监督”识别流程**。

- **核心思路**: 新方案将不再依赖“盲猜”式的无监督分离。而是将用户提供的已知说话人声纹样本作为“锚点”和“标准答案”，在转录流程的早期就介入，直接判断每一段语音的归属。
- **验收标准**:
    - 新方案生成的SRT文件中，说话人标签分配错误率应显著低于当前水平（目标 > 95%）。
    - 整个处理流程应是稳定、可复现的。
- **约束条件**:
    - 优先在现有技术栈（PyTorch, whisper, pyannote等）内解决问题。
    - 用户暂时不提供手动校对的“标准答案”数据集，方案的有效性需要通过逻辑和初步测试结果来验证。

## 3. 下一步计划

我们将基于此共识，进入 **架构 (Architect)** 阶段，详细设计新方案的技术架构、数据流和关键实现步骤。
