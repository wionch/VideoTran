# AI 视频翻译及配音系统 - 开发文档

## 1. 引言
本文档为“AI视频翻译及配音系统”的详细技术和开发指南，旨在为开发人员提供系统架构、环境配置、执行流程和代码参考，以便于系统的部署、运行和二次开发。

## 2. 系统架构

### 2.1 核心组件
系统由一系列解耦的、可独立运行的Python脚本和AI模型库组成，通过文件I/O进行数据传递。

*   **音频分离**: `AudioCraft` - 用于从源音频中分离人声和背景声。
*   **语音转录与Diarization**: `whisperX` - 用于将人声音频转换为带时间戳和说话人标签的JSON文件。
*   **文本翻译**: `Gemini / DeepSeek (via Python SDK)` - 用于调用大语言模型API，实现文本的批量翻译。
*   **语音合成**: `IndexTTS` - 用于根据翻译文本和参照音色，生成新的语音。
*   **音视频处理 (核心)**: `FFmpeg` - 开源的音视频处理工具集，用于执行底层的视频操作，如移除字幕、合并音视频和烧录新字幕。
*   **音视频处理库 (辅助)**: `pydub` (音频) 和 `moviepy` (视频) - 用于音频片段的裁剪、合并以及视频处理流程中的辅助操作。

### 2.2 数据流
```
[原始视频.mp4]
     | (提取音轨)
     v
[原始音频.wav] -> AudioCraft -> [vocals.wav] + [no_vocals.wav]
     |
     +--> whisperX -> [vocals.json] (含时间戳和说话人)
          |
          +--> translate_texts.py -> [vocals_translated.json] (新增翻译文本)
               |
               +--> synthesize_speech.py (使用vocals.wav做参照) -> [./synthesized/*.wav] (语音片段)
                    |
                    +--> assemble_final_track.py (合并语音片段和no_vocals.wav) -> [final_dubbed_audio.wav]
                         |
                         +--> process_video.py (核心视频处理流程)
                              |
                              +--> (可选) 遮罩硬字幕 -> [temp_video_no_hardsubs.mp4]
                              |
                              +--> 移除软字幕 -> [temp_video_no_softsubs.mp4]
                              |
                              +--> 生成SRT字幕 -> [translated_subtitles.srt]
                              |
                              +--> 合并音频并烧录字幕 -> [最终视频.mp4]
```

## 3. 环境配置

### 3.1 系统依赖
*   **FFmpeg**: 必须在您的系统环境中安装并配置好 `FFmpeg`，确保可以从命令行直接调用。

### 3.2 Python环境
建议使用 `Python 3.9+` 的虚拟环境。

### 3.3 依赖安装
1.  **核心工具**: 请根据 `AudioCraft`, `whisperX`, `IndexTTS` 各自的官方文档完成其环境的安装和模型的下载。
2.  **辅助库**: 安装流程中Python脚本所需的库。
    ```bash
    pip install pydub moviepy
    ```
3.  **大语言模型SDK**: 根据您选择的模型进行安装。
    ```bash
    # 以Google Gemini为例
    pip install google-generativeai
    ```

### 3.4 API密钥配置
在 `translate_texts.py` 脚本中或通过环境变量配置您的大语言模型API密钥。

## 4. 端到端执行流程

整个流程被拆分为多个独立的Python脚本，请按顺序执行。

### **第一部分：音频处理 (步骤 1-5)**

*这部分的脚本 (`translate_texts.py`, `synthesize_speech.py`, `assemble_final_track.py`) 与之前的版本保持一致，请参考原始文档。核心目标是生成 `final_dubbed_audio.wav` 和 `vocals_translated.json`。*

---


### **第二部分：视频处理 (步骤 6)**

#### 步骤 6: 移除字幕、合并音轨并烧录新字幕

运行 `process_video.py` 脚本。此脚本经过重构，利用 `FFmpeg` 实现更强大的视频处理功能，包括（可选的）硬字幕遮罩、软字幕移除，以及最终的音视频合并与字幕烧录。

**重要配置**: 在运行前，请打开 `process_video.py` 脚本并修改以下变量：
*   `ORIGINAL_VIDEO_PATH`: 指向您的原始视频文件。
*   `HARD_SUB_COORDS`: **（可选）** 如果需要移除视频中内嵌的硬字幕，请在此处提供字幕区域的精确坐标 `[x1, y1, x2, y2]` (左上角和右下角)。如果不需要，请将其设置为 `None`。

**脚本: `process_video.py` (已更新)**
```python
import json
import os
import subprocess
from moviepy.editor import VideoFileClip, AudioFileClip

# --- （可选）硬字幕处理函数 ---
def mask_hard_subtitles(video_path, output_path, coords):
    """使用 FFmpeg 的 delogo 过滤器对指定区域进行简单的模糊遮罩"""
    print(f"正在对坐标 {coords} 应用硬字幕遮罩...")
    x1, y1, x2, y2 = coords
    w = x2 - x1
    h = y2 - y1
    # 注意：FFmpeg的坐标系原点在左上角
    command = [
        'ffmpeg',
        '-i', video_path,
        '-vf', f"delogo=x={x1}:y={y1}:w={w}:h={h}", # 使用delogo过滤器进行模糊
        '-c:a', 'copy', # 复制音轨
        '-y',
        output_path
    ]
    # 对于更高级的AI Inpainting，这里会调用相应的Python库和模型
    subprocess.run(command, check=True, capture_output=True)
    print(f"已生成遮罩版视频: {output_path}")
    return output_path

# --- 其他函数 (format_time, create_srt_from_json, 等) ---
def format_time(seconds):
    """将秒转换为 SRT 时间格式 (HH:MM:SS,ms)"""
    millisec = int((seconds - int(seconds)) * 1000)
    seconds = int(seconds)
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return f"{h:02d}:{m:02d}:{s:02d},{millisec:03d}"

def create_srt_from_json(json_path, srt_path):
    """从翻译后的 JSON 数据生成 SRT 字幕文件"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    translated_texts = [seg['translated_text'] for seg in data['segments']]

    with open(srt_path, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(data['segments']):
            start_time = format_time(segment['start'])
            end_time = format_time(segment['end'])
            text = translated_texts[i]
            f.write(f"{i+1}\n")
            f.write(f"{start_time} --> {end_time}\n")
            f.write(f"{text}\n\n")
    print(f"SRT 字幕文件已生成: {srt_path}")

def remove_soft_subtitles(video_path, output_path):
    """使用 FFmpeg 移除视频的软字幕轨道"""
    print(f"正在移除软字幕: {video_path}")
    command = [
        'ffmpeg',
        '-i', video_path,
        '-c', 'copy',         # 直接复制视频和音频流，不做重新编码
        '-sn',              # 移除所有字幕轨道 (Subtitle None)
        '-y',               # 覆盖输出文件
        output_path
    ]
    subprocess.run(command, check=True, capture_output=True)
    print(f"已生成无字幕版视频: {output_path}")
    return output_path

def burn_subtitles_and_add_audio(video_path, audio_path, srt_path, output_path):
    """合并音视频，并使用 FFmpeg 烧录字幕"""
    print("开始合并音视频并烧录字幕...")
    temp_video_with_audio = "temp_video_with_audio.mp4"
    
    # 使用 moviepy 设置新音轨 (moviepy 在处理音频方面有时更稳定)
    video_clip = VideoFileClip(video_path)
    audio_clip = AudioFileClip(audio_path)
    final_clip = video_clip.set_audio(audio_clip)
    final_clip.write_videofile(temp_video_with_audio, codec='libx264', audio_codec='aac')

    # 使用 FFmpeg 烧录字幕 (FFmpeg 在字幕处理上更强大和可靠)
    print(f"使用 FFmpeg 烧录字幕: {srt_path}")
    # 注意：Windows下字幕文件的路径可能需要特殊处理
    escaped_srt_path = srt_path.replace('\\', '/').replace(':', '\\:')
    command = [
        'ffmpeg',
        '-i', temp_video_with_audio,
        '-vf', f"subtitles='{escaped_srt_path}'", # 指定字幕文件
        '-c:a', 'copy', # 复制音频流
        '-c:v', 'libx264',
        '-preset', 'fast',
        '-y',
        output_path
    ]
    subprocess.run(command, check=True, capture_output=True)
    os.remove(temp_video_with_audio) # 删除临时文件
    print(f"--- 视频处理完成！最终文件: {output_path} ---")

if __name__ == '__main__':
    # --- 1. 文件路径定义 ---
    ORIGINAL_VIDEO_PATH = "path/to/your/original_video.mp4" # <--- 修改为你的原视频路径
    DUBBED_AUDIO_PATH = "final_dubbed_audio_v3.wav"
    JSON_TRANSCRIPT_PATH = "transcribed_output/vocals_translated.json"
    
    # --- 2. (可选) 定义硬字幕坐标并处理 ---
    # 如果需要移除硬字幕，请在此处提供字幕区域的坐标 [x1, y1, x2, y2]
    # x1, y1 是左上角坐标; x2, y2 是右下角坐标。设为 None 则跳过此步骤。
    HARD_SUB_COORDS = [0, 880, 1920, 1020] # 示例: 为1080p视频底部字幕区
    # HARD_SUB_COORDS = None

    video_to_process = ORIGINAL_VIDEO_PATH
    if HARD_SUB_COORDS:
        video_to_process = mask_hard_subtitles(
            ORIGINAL_VIDEO_PATH, 
            "temp_video_no_hardsubs.mp4", 
            HARD_SUB_COORDS
        )

    # --- 3. 移除软字幕 ---
    # 注意：输入是上一步处理完的视频（如果执行了的话）
    video_after_softsubs = remove_soft_subtitles(
        video_to_process, 
        "temp_video_no_softsubs.mp4"
    )

    # --- 4. 生成SRT字幕文件 ---
    SRT_OUTPUT_PATH = "translated_subtitles.srt"
    create_srt_from_json(JSON_TRANSCRIPT_PATH, SRT_OUTPUT_PATH)

    # --- 5. 合成最终视频 ---
    FINAL_VIDEO_OUTPUT_PATH = "final_video_with_new_audio_and_subs.mp4"
    burn_subtitles_and_add_audio(
        video_after_softsubs, 
        DUBBED_AUDIO_PATH, 
        SRT_OUTPUT_PATH, 
        FINAL_VIDEO_OUTPUT_PATH
    )
```

## 5. 未来扩展
*   **组件替换**: 可通过修改对应的脚本（如`translate_texts.py`）来更换不同的LLM或TTS服务。
*   **性能优化**: `video_tran/tts_generator/run.py` 模块（旧称 `synthesize_speech.py`）已于近期重构。当前版本采用**一次性加载模型**的策略，在处理大量片段时，其执行效率相比旧版（为每个片段重启进程）有上百倍的提升，有效解决了性能瓶颈。
*   **工作流引擎**: 为实现更高级的自动化，可将此系列脚本集成到Airflow、Prefect等工作流管理引擎中。
*   **高级硬字幕修复**: 将 `mask_hard_subtitles` 函数中的简单模糊替换为基于AI的视频修复（Inpainting）模型，以获得更好的视觉效果。